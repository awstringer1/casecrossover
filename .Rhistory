hs <- list(i = integer(c(0,1,0,1,2)),p = integer(c(0,2,4,5,5)))
hs <- list(i = as.integer(c(0,1,0,1,2)),p = as.integer(c(0,2,4,5,5)))
hs
hessian_log_likelihood(rep(0,3),model_data1,hs)
hessian_log_likelihood(rep(0,3),model_data1,hs)@x
hessian_log_likelihood_x(rep(0,3),model_data1,hs)
hessian_log_likelihood_x(rep(0,3),model_data1)
hx1
devtools::test()
devtools::test()
devtools::test()
hessian_log_likelihood(rep(0,3),model_data1)@x
hessian_log_likelihood_x(rep(0,3),model_data1)
devtools::test()
devtools::test()
h1
devtools::test()
hs <- list(i = as.integer(c(0,1,0,1,2)),p = as.integer(c(0,2,4,5,5)))
hx1_1_1 <- g1_1[1]*(1 - g1_1[1])
hx1_1_2 <- -g1_1[1]*g1_2[1]
hx1_2_2 <- g1_1[2]*(1 - g1_1[2])
hx1_3_3 <- g1_1[3]*(1 - g1_1[3])
hx1 <- c(hx1_1_1,hx1_1_2,hx1_2_2,hx1_3_3)
h1 <- new("dgCMatrix")
h1@i <- hs$i
h1@p <- hs$p
h1@Dim <- as.integer(rep(length(hs$p) - 1,2))
h1@x <- numeric(length(h1@i))
h1 <- as(h1,'symmetricMatrix')
h1@x <- hx1
h1 <- as(h1,'dgCMatrix')
h1
identical(hessian_log_likelihood(rep(0,3),model_data1))
identical(hessian_log_likelihood(rep(0,3),model_data1),h1)
?identical
identical(hessian_log_likelihood(rep(0,3),model_data1),h1,ignore.environment = TRUE)
?expect_identical
identical(hessian_log_likelihood(rep(0,3),model_data1),h1,ignore.environment = TRUE)
identical(hessian_log_likelihood(rep(0,3),model_data1),h1,ignore.environment = TRUE)
all.equal(hessian_log_likelihood(rep(0,3),model_data1),h1,ignore.environment = TRUE)
str(h1)
str(hessian_log_likelihood(rep(0,3),model_data1))
attr(h1)
attributes(h1)
attributes(hessian_log_likelihood(rep(0,3),model_data1))
devtools::test()
devtools::test()
W1
devtools::test()
devtools::test()
devtoo
devtools::test()
hw1 <- 2 * h1
h1
hw1
devtools::test()
devtools::test()
devtools::test()
model_data1$Wd
model_data1$case_
model_data1$case_days
get_casedays(model_daya1)
get_casedays(model_data1)
### Likelihood ###
# This script contains functions for computing the likelihood for the casecrossover model
# This includes the gradient and hessian and any helper functions
# Most (all?) of these functions are not exported and are hidden from users, though
# they are still documented.
#' Prepare the data for computing the log-likelihood
#'
#' @description Take in the parameter vector and the model_data and prepare the data
#' for computing the log-likelihood. The log-likelihood is not permutation-invariant,
#' which is a fancy math way of saying "it matters what order the parameters are in".
#' Inside model_data, you have a named vector of control_days containing each subject's
#' id and the number of control days they have in the data. The data is sorted in
#' ascending order of id, in blocks with all control days followed by the case day.
#' This function splits the parameter vector W into a list, with one item per id,
#' containing the ordered elements of W corresponding to each subjects' control and
#' case days.
#'
#' @param W Parameter vector. First n elements are eta, then Gamma and beta.
#' @param model_data A list of class "cc_modeldata" as returned by model_setup().
#'
#' @return A list with n items (where n is the number of subjects) containing vectors
#' of the parameters corresponding to each subject's control and case days.
#'
prep_data_for_log_lik <- function(W,model_data) {
# Get delta
delta <- W[1:model_data$Nd]
# Split delta into a list containing the values for each control day for each subject
split(delta,as.numeric(rep(names(model_data$control_days),model_data$control_days)))
}
#' Get a vector of case day weights from the model_data
#'
#' @description Get a vector of case day weights, repeated once for every parameter
#' ascribed to each subject. Used inside gradient and hessian calculations as weights.
#'
#' @return A numeric vector of case day weights
#'
#' @param model_data A list of class "cc_modeldata" as returned by model_setup().
#'
get_casedays <- function(model_data) unname(rep(model_data$case_days,model_data$control_days))
#' Compute the log-likelihood for the model at a given parameter configuration
#'
#' @description Compute the model log-likelihood for a given set of parameters.
#'
#' @param W Parameter vector. First n elements are eta, then Gamma and beta.
#' @param model_data A list of class "cc_modeldata" as returned by model_setup().
#'
#' @return A number, the log-likelihood.
#'
#'
log_likelihood <- function(W,model_data) {
# Split the parameter vector
deltasplit <- prep_data_for_log_lik(W,model_data)
# Helper function to compute the log(1 + sum(exp(-delta))) for each person.
# Use logsumexp to avoid underflow in the individual delta terms inside the sum
# Then exponentiate the result- if the WHOLE SUM is so close to -Inf that exponentiating
# it gives nearly zero, it's not a problem, since 1 + x basically equals 1 in that case
# anyways. Underflow is only a problem when adding the exp(-delta_it) terms.
compute_loglik_term <- function(deltavec) {
# deltavec is a vector of deltas for each person
# Note we are adding both minus signs in here.
-log(1 + exp(matrixStats::logSumExp(-deltavec)))
}
# Now apply that function to the whole list and sum the result to get the answer
llterms <- purrr::map(deltasplit,compute_loglik_term) %>% purrr::reduce(c)
sum(llterms * model_data$case_days)
}
#' Gadient of the case-crossover log-likelihood.
#'
#' @description Compute the gradient of the log likelihood with respect to W = (delta,gamma,beta).
#' The gamma and beta parts are zero.
#'
#' @return A numeric vector containing the gradient. It's not stored as a sparseVector,
#' because it's (mathematically) not sparse. I guess the end is all zeroes though, so maybe
#' we should change this...?
#'
#' @inheritParams log_likelihood
#'
grad_log_likelihood <- function(W,model_data) {
# Split the parameter vector
deltasplit <- prep_data_for_log_lik(W,model_data)
casedays <- get_casedays(model_data)
# Helper to compute the gradient term
compute_gradient_term <- function(deltavec) {
# Compute the denominator
denom <- 1 + exp(matrixStats::logSumExp(-deltavec))
# Return exp(-deltavec)/denom
exp(-deltavec)/denom
}
# The gradient is the concatenation of all these (vector) terms,
# plus zeroes on the end to make it as long as W
gradient_front <- purrr::map(deltasplit,compute_gradient_term) %>% purrr::reduce(c)
gradient_back <- rep(0,length(W) - length(gradient_front))
c(gradient_front * casedays,gradient_back)
}
#' Compute the negated Hessian of the log-likelihood
#'
#' @description Function to implement the (sparse) NEGATED hessian of the case-crossover log-likelihood
#' This is MINUS the second derivative. Because that's what's used in the paper.
#' So remember when optimizing: provide the NEGATIVE of log_likelihood and gradient_...,
#' but provide the POSITIVE of this function.
#'
#' Two functions are written. hessian_log_likelihood_x() provides the ELEMENTS of the hessian only
#' This is very fast (134 milliseconds on average for the air pollution example)
#' hessian_log_likelihood_structure() brute-force computes the hessian using bdiag,
#' which is an order of magnitude slower (around a second on average). But the structure
#' never changes, so we only need to compute this once.
#' hessian_log_likelihood() computes the hessian, but you can supply it the structure and it
#' will basically just wrap hessian_log_likelihood_x(). Much faster.
#'
#' @inheritParams log_likelihood
#'
#' @return A sparse matrix inheriting from class CsparseMatrix containing the negated
#' Hessian of the log-likelihood.
#'
hessian_log_likelihood_structure <- function(W,model_data) {
# Arguments: see log_likelihood
# Returns: a list with i and p for the sparse hessian. NOTE: currently does it
# for dgCMatrix- i.e. not symmetric. I couldn't figure out how to do it for symmetric.
# Still fast and memory efficient.
# Returns the structure as (i,p), in column-compressed format. To get (i,j) triplet
# format do triplet = TRUE
# Split the parameter vector
deltasplit <- prep_data_for_log_lik(W,model_data)
# Helper function to create the blocks
# Takes in a vector of deltasplit and returns a block
# These blocks are dense.
compute_hessian_block <- function(deltavec) {
denom <- 1 + exp(matrixStats::logSumExp(-deltavec))
expdeltavecscaled <- exp(-deltavec)/denom
if (length(expdeltavecscaled) == 1) return(expdeltavecscaled * (1 - expdeltavecscaled))
diag(expdeltavecscaled) - expdeltavecscaled %o% expdeltavecscaled
}
# Create a list of blocks
blocklist <- purrr::map(deltasplit,compute_hessian_block)
# Then create a big zero matrix of dimension equal to length(W) - sum(length(deltasplit))
blocklist <- c(blocklist,Matrix(0,model_data$Wd - model_data$Nd,model_data$Wd - model_data$Nd))
# Final result is a big block diagonal matrix consisting of the created blocks
# concatenated with the zero block. The Matrix::bdiag function is REALLY slow for
# the case of many small dense matrices, as we have here (see docs).
# UPDATE: changed to symmetric structure.
structure <- bdiag(blocklist)
# structure <- forceSymmetric(bdiag(blocklist))
return(list(i = structure@i,p = structure@p))
}
#' @rdname hessian_log_likelihood_structure
#'
hessian_log_likelihood_x <- function(W,model_data) {
deltasplit <- prep_data_for_log_lik(W,model_data)
casedays <- get_casedays(model_data)
# Helper function to create the blocks
# Takes in a vector of deltasplit and returns a block
# These blocks are dense.
compute_hessian_block <- function(deltavec) {
denom <- 1 + exp(matrixStats::logSumExp(-deltavec))
dveclen <- length(deltavec)
expdeltavecscaled <- exp(-deltavec)/denom
if (length(expdeltavecscaled) == 1) return(expdeltavecscaled * (1 - expdeltavecscaled))
outmat <- diag(expdeltavecscaled) - expdeltavecscaled %o% expdeltavecscaled
outmat[upper.tri(outmat,diag = TRUE)]
}
purrr::map2(deltasplit,casedays,
~compute_hessian_block(.x) * .y) %>% purrr::reduce(c)
}
#' @rdname hessian_log_likelihood_structure
#' @param structure Optional. A list returned by hessian_log_likelihood_structure()
#' which contains the sparse structure of the hessian. Computing this is the slow
#' part, but only needs to be done once.
hessian_log_likelihood <- function(W,model_data,structure=NULL) {
# structure is a list containing elements i and p
# If not provided, will call hessian_log_likelihood_structure()
if (is.null(structure)) {
# Column-compressed format
structure <- hessian_log_likelihood_structure(W,model_data)
}
out <- new("dgCMatrix")
out@i <- structure$i
out@p <- structure$p
out@Dim <- as.integer(rep(length(structure$p) - 1,2))
out@x <- numeric(length(out@i))
out <- as(out,'symmetricMatrix')
out@x <- hessian_log_likelihood_x(W,model_data)
return(as(out,'dgCMatrix'))
}
hessian_log_likelihood_x(W1,model_data1)
W <- W1
model_data <- model_data1
# These blocks are dense.
compute_hessian_block <- function(deltavec) {
denom <- 1 + exp(matrixStats::logSumExp(-deltavec))
dveclen <- length(deltavec)
expdeltavecscaled <- exp(-deltavec)/denom
if (length(expdeltavecscaled) == 1) return(expdeltavecscaled * (1 - expdeltavecscaled))
outmat <- diag(expdeltavecscaled) - expdeltavecscaled %o% expdeltavecscaled
outmat[upper.tri(outmat,diag = TRUE)]
}
deltasplit <- prep_data_for_log_lik(W,model_data)
casedays <- get_casedays(model_data)
deltasplit
casedays
deltasplit <- prep_data_for_log_lik(W,model_data)
casedays <- model_data$case_days # Note: this is intentionally different than for the gradient.
casedays
hessian_log_likelihood_x <- function(W,model_data) {
deltasplit <- prep_data_for_log_lik(W,model_data)
casedays <- model_data$case_days # Note: this is intentionally different than for the gradient.
# Helper function to create the blocks
# Takes in a vector of deltasplit and returns a block
# These blocks are dense.
compute_hessian_block <- function(deltavec) {
denom <- 1 + exp(matrixStats::logSumExp(-deltavec))
dveclen <- length(deltavec)
expdeltavecscaled <- exp(-deltavec)/denom
if (length(expdeltavecscaled) == 1) return(expdeltavecscaled * (1 - expdeltavecscaled))
outmat <- diag(expdeltavecscaled) - expdeltavecscaled %o% expdeltavecscaled
outmat[upper.tri(outmat,diag = TRUE)]
}
purrr::map2(deltasplit,casedays,
~compute_hessian_block(.x) * .y) %>% purrr::reduce(c)
}
devtools::test()
Q_matrix_linear <- function(model_data,tau=exp(12)) {
Sbinv <- Diagonal(ncol(model_data$Xd),exp(model_data$beta_logprec))
rbind(
cbind(tau*model_data$lambdainv,-tau*model_data$lambdainv %*% model_data$Xd),
cbind(-tau*t(model_data$Xd) %*% model_data$lambdainv,Sbinv + tau*crossprod(crossprod(model_data$lambdainv,model_data$Xd),model_data$Xd))
)
}
Q_matrix_linear(model_data1)
Q_matrix_linear(model_data1) * exp(-12)
Q_matrix_linear(model_data1,tau = 1)
model_data1$lambdainv
model_data1$beta_logprec
exp(model_data1$beta_logprec)
log(.001)
devtools::test()
Q_matrix_linear(model_data1,tau = exp(12))
exp(-12) * Q_matrix_linear(model_data1,tau = 1)
devtools::test()
Q_matrix_linear(model_data1,tau = 1)
Q_matrix_linear(model_data1,tau = exp(12)) * exp(-12)
Q_matrix_linear(model_data1,tau = 1) %>% isSymmetric()
devtools::test()
devtools::test()
devtools::test()
devtools::test()
Q_matrix_rw2_one_component <- function(theta,model_data,covariate) {
u <- sort(unique(model_data$A[[covariate]]$u))
ul <- length(u)
du <- diff(u)
H <- Matrix::bandSparse(n = ul,
diagonals = list(
c(1/du[-(ul-1)],0),
c(0,-(1/du[-(ul-1)] + 1/du[-1]),0),
c(0,1/du[-1])
),
k = c(-1,0,1))
AA <- Matrix::Diagonal(n = ul,x = c(2/du[1],2/(du[-(ul-1)] + du[-1]),2/du[(ul-1)]))
exp(theta) * forceSymmetric(crossprod(H,crossprod(AA,H)))
}
Q_matrix_rw2_one_component(0,model_data3,"x")
model_data3$M
length(unique(sampledata$x))
model_data7 <- model_setup(case1 ~ s(x) + s(x2) + strata(id),sampledata,controlsmooth)
model_data8 <- model_setup(case2 ~ s(x) + s(x2) + strata(id),sampledata,controlsmooth)
sampledata <- tibble(
id = c(1,1,1,2,2),
case1 = c(0,0,1,0,1),
case2 = c(0,0,2,0,2),
x = c(1,2,3,1,2),
x2 = c(4,5,6,4,5)
)
model_data7 <- model_setup(case1 ~ s(x) + s(x2) + strata(id),sampledata,controlsmooth)
model_data8 <- model_setup(case2 ~ s(x) + s(x2) + strata(id),sampledata,controlsmooth)
model_data7$model_elements
devtools::test()
source('~/phd/projects/case-crossover/rpkg/casecrossover/tests/testthat/prep-sample-data.R', echo=TRUE)
devtools::test()
model_data7
devtools::test()
devtools::test()
devtools::test()
devtools::test()
devtools::test()
devtools::test()
devtools::test()
devtools::check()
devtools::check()
devtools::check()
devtools::check()
devtools::test()
devtools::test()
devtools::test()
devtools::test()
Q_matrix_rw2(0,model_data3)
model_data3$Wd
model_data5$Wd
model_data5$M
model_data5$model_elements
model_data5$control_days
devtools::test()
sampledata <- tibble(
id = c(1,1,1,2,2),
case1 = c(0,0,1,0,1),
case2 = c(0,0,2,0,2),
x = c(1,2,3,1,2),
x2 = c(4,5,6,4,5)
)
ff1 <- case1 ~ x + strata(id)
controlsmooth <- cc_control(smooth_prior = pc_prior(3,.75),
linear_constraints = create_linear_constraints(u = sampledata$x,
whichzero = 1,
nm = "x"))
model_data1 <- model_setup(case1 ~ x + strata(id),sampledata)
model_data2 <- model_setup(case2 ~ x + strata(id),sampledata)
model_data3 <- model_setup(case1 ~ s(x) + strata(id),sampledata,controlsmooth)
model_data4 <- model_setup(case2 ~ s(x) + strata(id),sampledata,controlsmooth)
model_data5 <- model_setup(case1 ~ x + s(x) + strata(id),sampledata,controlsmooth)
model_data6 <- model_setup(case2 ~ x + s(x) + strata(id),sampledata,controlsmooth)
model_data7 <- model_setup(case1 ~ s(x) + s(x2) + strata(id),sampledata,controlsmooth)
model_data8 <- model_setup(case2 ~ s(x) + s(x2) + strata(id),sampledata,controlsmooth)
model_data5$M
model_data5$p
model_data5$control_days
model_data5$Wd
controlsmooth$linear_constraints
formula <- case1 ~ x + s(x) + strata(id)
data <- sampledata
control <- controlsmooth
# TODO: document this.
model_data <- structure(list(), class = "ccmodeldata")
# Parse the formula
model_elements <- parse_formula(formula)
# Check that the smooth and strata terms exist in the data
# The linear terms will be passed to model.matrix, which has its own
# error checking.
extra_model_vars <- model_elements[c("smooth","strata")] %>% purrr::reduce(c)
if (!all(extra_model_vars %in% colnames(data))) {
missing_vars <- extra_model_vars[!(extra_model_vars %in% colnames(data))]
stop(paste0("The following variables were provided in the model formula but not in the data: ",
stringr::str_c(missing_vars,collapse = ", ")))
}
# Create the smooth terms- design matrix
Alist <- list()
if (length(model_elements$smooth) > 0) {
for (nm in model_elements$smooth) {
Alist[[nm]] <- create_alist_element(data[[nm]])
}
}
model_data$A <- Alist
if (length(Alist) == 0) {
model_data$A <- NULL
model_data$M <- 0 # No smooth terms
} else {
model_data$M <- Alist %>% purrr::map("A") %>% purrr::map(ncol) %>% purrr::reduce(sum)
}
# TODO: when implementing the linear constraint part, where one element of the
# precision matrix is set to 0, make sure to make the appropriate reduction in M
# Number of subjects
n <- length(unique(data[model_elements$strata]))
# Linear terms
if (length(model_elements$linear) == 0) {
model_data$X <- NULL
model_data$p <- 0 # No linear terms
} else {
model_data$X <- Matrix::sparse.model.matrix(model_elements$linear_formula,data = data)
model_data$p <- ncol(model_data$X)
# Safety check: ncol(X) > 0.
if (ncol(model_data$X) == 0) {
model_data$X <- NULL
model_data$p <- 0 # No linear terms
}
}
control_days <- data %>%
dplyr::arrange(.data[[model_elements$strata]],.data[[model_elements$response]]) %>%
dplyr::filter(.data[[model_elements$response]] == 0) %>%
dplyr::group_by(.data[[model_elements$strata]]) %>%
dplyr::summarize(control_days = n())
# Create the vector of case days
# A named vector where the names are the subject ids and the values are the number
# of control days that each has in the data
case_days <- data %>%
dplyr::arrange(.data[[model_elements$strata]],.data[[model_elements$response]]) %>%
dplyr::filter(.data[[model_elements$response]] != 0) %>%
dplyr::rename(case_days = .data[[model_elements$response]])
model_data$control_days <- control_days$control_days
model_data$case_days <- case_days$case_days
names(model_data$control_days) <- data[[model_elements$strata]] %>% unique() %>% sort()
names(model_data$case_days) <- data[[model_elements$strata]] %>% unique() %>% sort()
model_data$n <- length(model_data$case_days)
model_data$Nd <- sum(model_data$control_days)
model_data$Ne <- model_data$Nd + model_data$n
model_data$Wd <- model_data$M + model_data$p + model_data$Nd
model_data$Wdf <- model_data$M + model_data$p + model_data$Ne
model_data$M
model_data$p
model_data$Nd
model_data$Wd
priorvalid <- TRUE
if (model_data$M > 0) {
validate_prior_distribution(control$smooth_prior,verbose)
if (!priorvalid) stop("Please specify a valid prior for your smooth terms.")
# If it's valid, grab the actual function call and set the parameters
model_data$theta_logprior <- function(theta) {
callargs <- as.list(c(theta = theta,control$smooth_prior$params))
do.call(supported_prior_distributions(control$smooth_prior$name)$call,callargs)
}
} else {
model_data$theta_logprior <- function(theta) {force(theta); return(0)} # Placeholder
}
# log(precision) for prior on beta. Specified in control
model_data$beta_logprec <- control$beta_prior_logprec
# Differenced matrices:
model_data$diffmat <- create_diff_matrix(model_data$control_days)
model_data$lambdainv <- create_full_dtcp_matrix(model_data$control_days)
if (model_data$M > 0) {
for (nm in names(model_data$A)) {
model_data$A[[nm]]$Ad <- model_data$diffmat %*% model_data$A[[nm]]$A
}
}
if (model_data$p > 0) {
model_data$Xd <- model_data$diffmat %*% model_data$X
}
length(model_elements$smooth) > 0
length(control$linear_constraints) == 0
model_data$linear_constraints <- NULL
if (length(model_elements$smooth) > 0) {
if (length(control$linear_constraints) == 0) {
warning("Smooth terms, but no linear constraints, specified. You should add one or more constraints. See create_linear_constraints().")
} else {
# Take the first one and use it to set one to zero
nm <- model_elements$smooth[1]
model_data$vectorofcolumnstoremove <- control$linear_constraints[[nm]]$whichzero
# Adjust M
model_data$M <- model_data$M - 1
model_data$Wd <- model_data$M + model_data$p + model_data$Nd
model_data$Wdf <- model_data$M + model_data$p + model_data$Ne
# Remove this constraint from the list of constraints
# ACTUALLY: do I have to...? If I add it back in later I don't think it matters. We'll see.
}
}
model_data$linear_constraints
model_data$Wd
model_data$M
devtools::test()
devtools::test()
model_data5$M
model_data5$p
model_data5$Nd
model_data5$Wd
